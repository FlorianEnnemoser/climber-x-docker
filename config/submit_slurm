#!/bin/bash
#SBATCH --qos=<QOS>
#SBATCH --partition=<PARTITION>
#SBATCH --time=<WALL>
#SBATCH --job-name=<JOBNAME>
#SBATCH --account=<ACCOUNT>
#SBATCH --mail-user=<EMAIL>
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-type=REQUEUE
#SBATCH --output=./out.out
#SBATCH --error=./out.err

### OMP Specific settings ###
#SBATCH --nodes=1
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=<OMP>
#SBATCH --no-requeue

export OMP_PROC_BIND=true           # make sure our threads stick to cores
export OMP_NUM_THREADS=<OMP>        # matches how many cpus-per-task we asked for
export OMP_NESTED=false
export OMP_STACKSIZE=512M
export MKL_NUM_THREADS=<OMP>

# OpenMP and srun, both need to know the number of CPUs per task
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK

## for HPC2024, other options
#export OMP_SCHEDULE=static
#export OMP_DYNAMIC=false
#export OMP_THREAD_LIMIT=256
#export OMP_NESTED=FALSE
#export OMP_STACKSIZE=256M
#export OMP_PLACES=cores
#export OMP_DISPLAY_AFFINITY=TRUE

### DONE OMP Specific settings ###

ulimit -s unlimited
#ulimit -c unlimited
#ulimit -s unlimited
#ulimit -d unlimited
#ulimit -m unlimited
ulimit -v unlimited
#ulimit -f unlimited
ulimit -a

export GMON_OUT_PREFIX=./gmon
export LD_LIBRARY_PATH=${NETCDF_FORTRANROOT}/lib:${NETCDF_CROOT}/lib:$LD_LIBRARY_PATH
module load cdo

# Run the job
srun <CMD> 
